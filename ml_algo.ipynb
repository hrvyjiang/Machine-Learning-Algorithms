{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "### Time: O(2mn+2mn²+n³)\n",
    "### Space: O(n * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression:\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coefficients = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "\n",
    "        self.coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        if self.fit_intercept:\n",
    "            self.intercept = self.coefficients[0]\n",
    "            self.coefficients = self.coefficients[1:]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "        return X @ np.concatenate([[self.intercept], self.coefficients])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "### Time: O(n_iterations * n_samples * n_features)\n",
    "### Space: O(n_samples * n_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_regression:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.losses = []\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # binary cross entropy\n",
    "        epsilon = 1e-9\n",
    "        y1 = y_true * np.log(y_pred + epsilon)\n",
    "        y2 = (1 - y_true) * np.log(1 - y_pred + epsilon)\n",
    "        return -np.mean(y1+y2)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        #Gradient Descent\n",
    "        for _ in range(self.n_iters):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw \n",
    "            self.bias -= self.learning_rate * db     \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        return [1 if i > threshold else 0 for i in y_predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(predictions, y_test):\n",
    "    if len(predictions) != len(y_test):\n",
    "        return ValueError\n",
    "    # Calculate Accuracy, Precision, Recall, Specificity\n",
    "    TP = 0\n",
    "    TN = 0 \n",
    "    FP = 0 \n",
    "    FN = 0 \n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == y_test[i]:\n",
    "            if predictions[i] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if predictions[i] == 1:\n",
    "                FP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "                \n",
    "    # Accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
    "    Accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
    "\n",
    "    # Precision\n",
    "    Precision = TP / (TP+FP)\n",
    "\n",
    "    # Recall\n",
    "    Recall = TP / (TP+FN)\n",
    "\n",
    "    # Specificity\n",
    "    Specificity = TN/(TN+FP)\n",
    "\n",
    "    return Accuracy, Precision, Recall, Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors\n",
    "### n= number of training examples, m=number of test samples, d= number of dimensions of the data, k= number of neighbors\n",
    "### Time: training = O(1), predict w/ heap/argsort  = O(m * n * log k)\n",
    "### Space: train: O(n * d), predict: O(m*d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from collections import Counter\n",
    "from time import time\n",
    "\n",
    "\n",
    "class knn_classifier:\n",
    "    def __init__(self, k=5, algorithm = 'heapq'):\n",
    "        self.k = k\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2)**2))\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        if self.algorithm == 'heapq':\n",
    "            for x_test in X_test:\n",
    "                heap = []\n",
    "                for i, x_train in enumerate(self.X_train):\n",
    "                    distances = self.euclidean_distance(x_test, x_train)\n",
    "                    if len(heap) < self.k:\n",
    "                        heapq.heappush(heap, (-distances, self.y_train[i]))\n",
    "                    else:\n",
    "                        heapq.heappushpop(heap, (-distances, self.y_train[i]))\n",
    "                nearest_labels = [label  for j,label in heap]\n",
    "                most_common_label = Counter(nearest_labels).most_common(1)[0][0]\n",
    "                y_pred.append(most_common_label)\n",
    "        else: \n",
    "            for x_test in X_test:\n",
    "                distances = [self.euclidean_distance(x_test, x_train) for x_train in self.X_train]\n",
    "                nearest_indices = np.argsort(distances)[:self.k]\n",
    "                nearest_labels = [self.y_train[i] for i in nearest_indices]\n",
    "                most_common_label = np.argmax(np.bincount(nearest_labels))\n",
    "                y_pred.append(most_common_label)\n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "### n= number of points in the Training set d=dimentionality of the data\n",
    "### Training Time: O(n*log(n)*d) \n",
    "### Space: O(n * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class TreeNode():\n",
    "    def __init__(self, data, feature_idx, feature_val, prediction_probs, information_gain) -> None:\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.prediction_probs = prediction_probs\n",
    "        self.information_gain = information_gain\n",
    "        self.feature_importance = self.data.shape[0] * self.information_gain\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def node_def(self) -> str:\n",
    "\n",
    "        if (self.left or self.right):\n",
    "            return f\"NODE | Information Gain = {self.information_gain} | Split IF X[{self.feature_idx}] < {self.feature_val} THEN left O/W right\"\n",
    "        else:\n",
    "            unique_values, value_counts = np.unique(self.data[:,-1], return_counts=True)\n",
    "            output = \", \".join([f\"{value}->{count}\" for value, count in zip(unique_values, value_counts)])            \n",
    "            return f\"LEAF | Label Counts = {output} | Pred Probs = {self.prediction_probs}\"\n",
    "        \n",
    "class DecisionTree():\n",
    "    def __init__(self, max_depth=4, min_samples_leaf=1, min_information_gain=0.0, numb_of_features_splitting=None, amount_of_say=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.numb_of_features_splitting = numb_of_features_splitting\n",
    "        self.amount_of_say = amount_of_say\n",
    "\n",
    "    def _entropy(self, class_probabilities: list) -> float:\n",
    "        return sum([-p * np.log2(p) for p in class_probabilities if p>0])\n",
    "    \n",
    "    def _class_probabilities(self, labels: list) -> list:\n",
    "        total_count = len(labels)\n",
    "        return [label_count / total_count for label_count in Counter(labels).values()]\n",
    "\n",
    "    def _data_entropy(self, labels: list) -> float:\n",
    "        return self._entropy(self._class_probabilities(labels))\n",
    "    \n",
    "    def _partition_entropy(self, subsets: list) -> float:\n",
    "        \"\"\"subsets = list of label lists (EX: [[1,0,0], [1,1,1])\"\"\"\n",
    "        total_count = sum([len(subset) for subset in subsets])\n",
    "        return sum([self._data_entropy(subset) * (len(subset) / total_count) for subset in subsets])\n",
    "    \n",
    "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
    "        \n",
    "        mask_below_threshold = data[:, feature_idx] < feature_val\n",
    "        group1 = data[mask_below_threshold]\n",
    "        group2 = data[~mask_below_threshold]\n",
    "\n",
    "        return group1, group2\n",
    "    \n",
    "    def _select_features_to_use(self, data: np.array) -> list:\n",
    "        feature_idx = list(range(data.shape[1]-1))\n",
    "\n",
    "        if self.numb_of_features_splitting == \"sqrt\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx))))\n",
    "        elif self.numb_of_features_splitting == \"log\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx))))\n",
    "        else:\n",
    "            feature_idx_to_use = feature_idx\n",
    "\n",
    "        return feature_idx_to_use\n",
    "        \n",
    "    def _find_best_split(self, data: np.array) -> tuple:\n",
    "        \"\"\"\n",
    "        Finds the best split (with the lowest entropy) given data\n",
    "        Returns 2 splitted groups and split information\n",
    "        \"\"\"\n",
    "        min_part_entropy = 1e9\n",
    "        feature_idx_to_use =  self._select_features_to_use(data)\n",
    "\n",
    "        for idx in feature_idx_to_use:\n",
    "            feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25))\n",
    "            for feature_val in feature_vals:\n",
    "                g1, g2, = self._split(data, idx, feature_val)\n",
    "                part_entropy = self._partition_entropy([g1[:, -1], g2[:, -1]])\n",
    "                if part_entropy < min_part_entropy:\n",
    "                    min_part_entropy = part_entropy\n",
    "                    min_entropy_feature_idx = idx\n",
    "                    min_entropy_feature_val = feature_val\n",
    "                    g1_min, g2_min = g1, g2\n",
    "\n",
    "        return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy\n",
    "\n",
    "    def _find_label_probs(self, data: np.array) -> np.array:\n",
    "        labels_as_integers = data[:,-1].astype(int)\n",
    "        # Calculate the total number of labels\n",
    "        total_labels = len(labels_as_integers)\n",
    "        # Calculate the ratios (probabilities) for each label\n",
    "        label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "        # Populate the label_probabilities array based on the specific labels\n",
    "        for i, label in enumerate(self.labels_in_train):\n",
    "            label_index = np.where(labels_as_integers == i)[0]\n",
    "            if len(label_index) > 0:\n",
    "                label_probabilities[i] = len(label_index) / total_labels\n",
    "\n",
    "        return label_probabilities\n",
    "\n",
    "    def _create_tree(self, data: np.array, current_depth: int) -> TreeNode:\n",
    "        # Check if the max depth has been reached (stopping criteria)\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        # Find best split\n",
    "        split_1_data, split_2_data, split_feature_idx, split_feature_val, split_entropy = self._find_best_split(data)\n",
    "        \n",
    "        # Find label probs for the node\n",
    "        label_probabilities = self._find_label_probs(data)\n",
    "\n",
    "        # Calculate information gain\n",
    "        node_entropy = self._entropy(label_probabilities)\n",
    "        information_gain = node_entropy - split_entropy\n",
    "        \n",
    "        # Create node\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain)\n",
    "\n",
    "        # Check if the min_samples_leaf has been satisfied (stopping criteria)\n",
    "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:\n",
    "            return node\n",
    "        # Check if the min_information_gain has been satisfied (stopping criteria)\n",
    "        elif information_gain < self.min_information_gain:\n",
    "            return node\n",
    "\n",
    "        current_depth += 1\n",
    "        node.left = self._create_tree(split_1_data, current_depth)\n",
    "        node.right = self._create_tree(split_2_data, current_depth)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def _predict_one_sample(self, X: np.array) -> np.array:\n",
    "        \"\"\"Returns prediction for 1 dim array\"\"\"\n",
    "        node = self.tree\n",
    "\n",
    "        # Finds the leaf which X belongs\n",
    "        while node:\n",
    "            pred_probs = node.prediction_probs\n",
    "            if X[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return pred_probs\n",
    "\n",
    "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model with given X and Y datasets\n",
    "        \"\"\"\n",
    "\n",
    "        # Concat features and labels\n",
    "        self.labels_in_train = np.unique(Y_train)\n",
    "        train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1)\n",
    "\n",
    "        # Start creating the tree\n",
    "        self.tree = self._create_tree(data=train_data, current_depth=0)\n",
    "\n",
    "        # Calculate feature importance\n",
    "        self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0)\n",
    "        self._calculate_feature_importance(self.tree)\n",
    "        # Normalize the feature importance values\n",
    "        self.feature_importances = {k: v / total for total in (sum(self.feature_importances.values()),) for k, v in self.feature_importances.items()}\n",
    "\n",
    "    def predict_proba(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted probs for a given data set\"\"\"\n",
    "        pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set)\n",
    "        return pred_probs\n",
    "\n",
    "    def predict(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted labels for a given data set\"\"\"\n",
    "        pred_probs = self.predict_proba(X_set)\n",
    "        preds = np.argmax(pred_probs, axis=1)\n",
    "        return preds    \n",
    "        \n",
    "    def _print_recursive(self, node: TreeNode, level=0) -> None:\n",
    "        if node != None:\n",
    "            self._print_recursive(node.left, level + 1)\n",
    "            print('    ' * 4 * level + '-> ' + node.node_def())\n",
    "            self._print_recursive(node.right, level + 1)\n",
    "\n",
    "    def print_tree(self) -> None:\n",
    "        self._print_recursive(node=self.tree)\n",
    "\n",
    "    def _calculate_feature_importance(self, node):\n",
    "        \"\"\"Calculates the feature importance by visiting each node in the tree recursively\"\"\"\n",
    "        if node != None:\n",
    "            self.feature_importances[node.feature_idx] += node.feature_importance\n",
    "            self._calculate_feature_importance(node.left)\n",
    "            self._calculate_feature_importance(node.right) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_classes = len(set(y))\n",
    "\n",
    "        # Check termination conditions\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or num_samples < self.min_samples_split or num_classes == 1:\n",
    "            return TreeNode(is_leaf=True, prediction=self._majority_class(y))\n",
    "\n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "\n",
    "        # Create the left and right child nodes\n",
    "        left_mask = X[:, best_feature] < best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        left_child = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_child = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        # Create the current node\n",
    "        return TreeNode(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gain = 0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(X, y, feature, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _information_gain(self, X, y, feature, threshold):\n",
    "        # Compute the entropy of the parent node\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        # Compute the entropy of the left and right child nodes\n",
    "        left_mask = X[:, feature] < threshold\n",
    "        right_mask = ~left_mask\n",
    "        left_entropy = self._entropy(y[left_mask])\n",
    "        right_entropy = self._entropy(y[right_mask])\n",
    "\n",
    "        # Compute the information gain\n",
    "        left_weight = np.sum(left_mask) / len(y)\n",
    "        right_weight = np.sum(right_mask) / len(y)\n",
    "        information_gain = parent_entropy - (left_weight * left_entropy + right_weight * right_entropy)\n",
    "\n",
    "        return information_gain\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum(probabilities * np.log(probabilities))\n",
    "\n",
    "    def _majority_class(self, y):\n",
    "        return np.argmax(np.bincount(y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_single(x, self.root) for x in X]\n",
    "\n",
    "    def _predict_single(self, x, node):\n",
    "        if node.is_leaf:\n",
    "            return node.prediction\n",
    "        elif x[node.feature] < node.threshold:\n",
    "            return self._predict_single(x, node.left)\n",
    "        else:\n",
    "            return self._predict_single(x, node.right)\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, is_leaf=False, prediction=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.is_leaf = is_leaf\n",
    "        self.prediction = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.30258509]\n",
      " [2.99573227]\n",
      " [3.40119738]\n",
      " [3.68887945]\n",
      " [3.91202301]] [4.60517019 5.70378247 6.2146081  6.55108034 6.80239476]\n",
      "Multiplicative coefficients: [3.91186704]\n",
      "Predicted values: [1228.25735218 1515.67393416 1818.47633529]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Prepare the data\n",
    "X = np.array([10, 20, 30, 40, 50])\n",
    "y = np.array([100, 300, 500, 700, 900])\n",
    "\n",
    "X_log = np.log(X).reshape(-1,1)\n",
    "y_log = np.log(y)\n",
    "print(X_log, y_log)\n",
    "# Train the multiplicative regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_log, y_log)\n",
    "\n",
    "# Interpret the coefficients\n",
    "coefficients = np.exp(model.coef_)\n",
    "print(\"Multiplicative coefficients:\", coefficients)\n",
    "\n",
    "# Make predictions\n",
    "X_new = np.array([60, 70, 80])\n",
    "X_new_log = np.log(X_new).reshape(-1,1)\n",
    "y_new_log = model.predict(X_new_log)\n",
    "y_new = np.exp(y_new_log)\n",
    "print(\"Predicted values:\", y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
